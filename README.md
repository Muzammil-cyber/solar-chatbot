# Solar Chatbot

A Flask-based chatbot that provides information about solar panels, installation, estimates, and maintenance services using local LLM (Phi-2) and semantic search.

## Features

- Local LLM integration using llama-cpp-python (Phi-2 model)
- Semantic search using sentence transformers
- Vector store for efficient knowledge retrieval
- RESTful API endpoint for chat interactions
- Pre-trained knowledge base about solar services
- Input cleaning and preprocessing

## Installation

1. Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Download the Phi-2 GGUF model:

The model file `phi2.gguf` should be placed in the `models/` directory. You can download it from Hugging Face or other sources.

4. Build the vector store:

```bash
python build_index.py
```

This will create a `vector_store.pkl` file containing the embedded knowledge chunks.

## Usage

### Running the Flask App

Start the Flask server:

```bash
python app.py
```

The server will run on `http://localhost:5000`

### API Usage

Send POST requests to `/chat` endpoint:

```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "How long does installation take?"}'
```

Response:

```json
{
  "response": "Our standard installation time is 7–10 working days."
}
```

### Testing

Run the test script to verify the chatbot functionality:

```bash
python test_chatbot.py
```

## Project Structure

```bash
solar-chatbot/
├── app.py                 # Flask application with chat endpoint
├── chatbot.py             # Core AI logic with LLM and semantic search
├── build_index.py         # Script to build vector store from knowledge
├── utils.py               # Input cleaning and preprocessing functions
├── data/
│   └── knowledge.txt      # Solar knowledge base
├── models/
│   └── phi2.gguf         # Local LLM model (Phi-2)
├── vector_store.pkl       # Precomputed embeddings (generated by build_index.py)
├── requirements.txt       # Python dependencies
├── test_chatbot.py       # Test script for verification
└── README.md             # This file
```

## Dependencies

- **Flask** - Web framework for the API
- **llama-cpp-python** - Python bindings for llama.cpp to run local LLMs
- **sentence-transformers** - Semantic search and embeddings
- **torch** - PyTorch for tensor operations (dependency of sentence-transformers)

## How It Works

1. **Knowledge Base**: Solar-related information is stored in `data/knowledge.txt`
2. **Vector Store**: The `build_index.py` script chunks the knowledge and creates embeddings using sentence transformers
3. **Semantic Search**: When a query is received, it's embedded and compared against the knowledge base
4. **LLM Generation**: The most relevant context is retrieved and passed to the local Phi-2 model for answer generation
5. **Response**: The LLM generates a contextual response based on the retrieved information

## Model Requirements

- **Phi-2 GGUF Model**: The chatbot uses the Phi-2 model in GGUF format for local inference
- **Sentence Transformer**: Uses "all-MiniLM-L6-v2" for creating embeddings
- **Vector Store**: Pre-computed embeddings stored in pickle format for fast retrieval

## Customization

To customize the knowledge base:

1. Update `data/knowledge.txt` with your solar company's information
2. Run `python build_index.py` to rebuild the vector store
3. Restart the Flask application

The chatbot is specifically configured for solar energy companies in Pakistan and can be adapted for other domains by updating the knowledge base and prompt engineering in `chatbot.py`.
